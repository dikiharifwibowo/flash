# -*- coding: utf-8 -*-
"""tugas5_datascience.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NaXMNyXPRa6nhpyDf0C7wte6Ov7aE5jX
"""

import tensorflow as tf
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from tensorflow.python.keras.layers import TimeDistributed, Dense, LSTM, Embedding, Dropout, Bidirectional, GlobalMaxPool1D
from tensorflow.python.keras.models import Model, Sequential
# -------- other packages ------------------ #
import sys
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
import pickle

from google.colab import files
uploaded = files.upload()

# import io
df = pd.read_csv('datasets.csv', index_col=0)
df = df.drop(df[df['transcription'].isna()].index)
df = df.drop(df[df['keywords'].isna()].index)

df.astype(str)
df.head()

df["keywords"].str.split(" ")

def hasNumbers(inputString):
	return any(char.isdigit() for char in inputString)

def tag_keywords(all_keywords):
	tokenizer = Tokenizer()
	tokenizer.fit_on_texts([all_keywords])
	all_keywords = [i for i in tokenizer.word_index.keys()]
	all_keywords = list(set(all_keywords))
	return all_keywords

def apply_max_threshold(cat):
  spl = cat.split(' ')

  res = []
  for i in spl:

    try:
      l = i.split(' ')
      if len(l) > 3:
        continue
    except:
      continue

    res.append(i)


  if len(res) > 45:
    return ','.join(res[:45])
  else:
    return ','.join(res)

df['keywords'] = df['keywords'].apply(apply_max_threshold)

df.head(5)

df['Sentence'] = df['transcription'].apply(lambda x: x.replace("â€“",""))
df['Keyword'] = df['keywords'].apply(lambda x: tag_keywords(x))

df['Keyword']

from pydantic import BaseModel

class Catat(BaseModel):
    catatan: str 
    class Config:
        schema_extra = {
            "example": {
                "catatan": "Consult for laparoscopic gastric", 
            }
        }

from fastapi import FastAPI
import pickle

app = FastAPI()

@app.on_event("startup")
def load_model():
    global model

@app.get('/')
def index():
    return {'message': 'This is the homepage of the API '}


@app.post('/keyword')
def get_keyword(data: Catat):
    received = data.dict()
    catatans = received['catatan']
    pred_keyword = tag_keywords(catatans)
    return {'keywords': pred_keyword}

from colabcode import ColabCode
server = ColabCode(port=8000, code=False)

server.run_app(app=app)

sentence_column = []
keyword_column = []
for index, row in df.iterrows():
	new_keywords = []
	sentence = row['Sentence']
	keywords = row['Keyword']
	tokenizer = Tokenizer()
	tokenizer.fit_on_texts([sentence])
	tokens = [i for i in tokenizer.word_index.keys()]
	for i in tokens:
		if i in keywords:
			if not hasNumbers(i):
				new_keywords.append(1)
		else:
			new_keywords.append(0)
	if sum(new_keywords) != 0:
		sentence_column.append(sentence)
		keyword_column.append(new_keywords)

keyword_column[0]

tokenizer = Tokenizer(oov_token = "<UNK>")
tokenizer.fit_on_texts(sentence_column)
X = tokenizer.texts_to_sequences(sentence_column)
X = pad_sequences(X, padding = "post", truncating = "post", maxlen = 25, value = 0)
y = pad_sequences(keyword_column, padding = "post", truncating = "post", maxlen = 25, value = 0)
# y = [tf.keras.utils.to_categorical(i, num_classes = 2) for i in y]

word_index = tokenizer.word_index

DIM = 45
EPOCHS = 11

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)
model = Sequential()
model.add(Embedding(len(word_index) + 1, DIM))
model.add(Bidirectional(LSTM(32, return_sequences = True, recurrent_dropout = 0.1)))
model.add(TimeDistributed(Dense(2, activation = "softmax")))
model.compile(loss="categorical_crossentropy", optimizer = "adam", metrics = ["accuracy"])
history = model.fit(X_train, np.array(y_train), batch_size = 32, epochs = EPOCHS, validation_split = 0.1)

from matplotlib import pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

test_output = model.predict(X_test)
test_output = np.argmax(test_output, axis = -1)


flattened_actual = (np.argmax(np.array(y_test), axis = -1)).flatten()
flattened_output = test_output.flatten()
print(classification_report(flattened_actual, flattened_output))

test_output = model.predict(X_test)
test_output = np.argmax(test_output, axis = -1)
test_output

print(test_output[0])
print(y_test[0].flatten())